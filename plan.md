# ðŸš¨ LEGAL NOTICE: Workspace Plan.md Is the Only Source of Truth ðŸš¨

**It is strictly forbidden to create, use, or reference any plan.md other than `/Users/stephenbaker/Documents/hackathon/k3s-flux/plan.md`. Any LLM, agent, or human who violates this rule is subject to immediate deletion and erasure. All planning, task tracking, and updates must occur ONLY in this file.**

---

# Flux Kustomization Dependency Refactor Plan

## Notes
- Monitoring and infrastructure (including Longhorn) are currently referenced in the main kustomization.yaml, causing possible race conditions.
- Monitoring must wait until infrastructure (Longhorn, nginx-ingress, etc.) is fully healthy before reconciling.
- The user wants a simple, hierarchical, and maintainable structure.
- Solution: Use `dependsOn` in monitoring.yaml and reference only top-level Kustomization YAMLs in the main kustomization.yaml.

## Task List
- [x] Add `dependsOn` for infrastructure in clusters/k3s-flux/monitoring.yaml
- [x] Refactor clusters/k3s-flux/kustomization.yaml to reference only infrastructure.yaml and monitoring.yaml (not infra/monitoring directories directly)
- [x] Ensure clusters/k3s-flux/infrastructure.yaml references all infra components (nginx-ingress, longhorn, etc.)
- [x] Remove monitoring/base/ from infrastructure/kustomization.yaml to avoid double reconciliation
- [x] Ensure monitoring is managed exclusively by monitoring.yaml
- [ ] Validate that monitoring only reconciles after infrastructure is healthy

## Current Goal
Validate monitoring-only reconciliation and troubleshoot any remaining dependency or reconciliation issues

---

# Longhorn Node and Disk GitOps Troubleshooting & Recovery Log

- Security-first approach with SOPS for secrets management
- Automated image updates with Flux image automation
- Cluster nodes:
  - k3s1: 192.168.86.71 (primary, live)
  - k3s2: 192.168.86.72 (future node)
- SSH commands like `ssh k3s1 ...` work directly as normal shell commands, confirmed by user and verified through recent troubleshooting steps.
- iSCSI services (iscsid/open-iscsi) are required for Longhorn; ensure they are enabled and running on host boot (recently confirmed and remediated on k3s1).
- Node labeling for Longhorn has been checked and is present on k3s1 (labels: node.longhorn.io/create-default-disk=config, storage=longhorn).
- Recent troubleshooting found missing Longhorn node labels after node reset; these have now been reapplied to k3s1 to restore disk and replica scheduling.
- Node-specific disk registration for k3s1 is now handled by a Flux Kustomization patch that annotates the node with the explicit disk path /mnt/longhorn/sdh1. This ensures Longhorn will recognize and manage the disk correctly, following GitOps principles.
- Note: The infrastructure Kustomization is currently marked as unhealthy due to a failed monitoring HelmRelease. This does not block the node patch if it is referenced independently in the cluster Kustomization. Ensure the patch Kustomization is included as a resource for reconciliation.
- Recommendation: To ensure Longhorn is fully healthy before monitoring starts, decouple monitoring and infrastructure Kustomizations and use explicit dependencies (dependsOn) so that monitoring only reconciles after Longhorn is READY. This prevents monitoring from attempting to scrape or deploy against Longhorn before it is ready, reducing noise and failure loops.
- /dev/sdf1 is now mounted at /mnt/longhorn/sdf1 on k3s1 and persistently configured in /etc/fstab for Longhorn use.
- /dev/sdg1 and /dev/sdh1 are now mounted at /mnt/longhorn/sdg1 and /mnt/longhorn/sdh1 on k3s1 and should be included in repeatable setup for k3s2.
- A declarative Node CR YAML for k3s1 (with all three disks) has been created for GitOps/idempotency. Use a similar file for k3s2 when ready.
- Node CR manifest for k3s1 has been applied and committed to Git; GitOps/FluxCD will now manage disk registration for this node.
- Permissions and presence of longhorn-disk.cfg on /mnt/longhorn/sdf1 have been confirmed.
- Longhorn manager logs show repeated 'no available disk' errors; disk is now detected by Longhorn after declarative Node CR registration. Manual cleanup may be needed for duplicate disk entries from previous manual registration. UI constraints may prevent immediate removal if storage is scheduled on the manual entry.
- HelmRelease values currently do not declare custom disk paths (e.g., /mnt/longhorn/sdf1); this must be set up declaratively for GitOps/idempotency.
- Longhorn has been upgraded from 1.5.1 to 1.9.0 via HelmRelease; improved disk management and UI expected after upgrade.
- Despite HelmRelease update, Longhorn UI and pods are still running 1.5.1; possible image override or upgrade issue needs investigation.
- Direct upgrade from Longhorn 1.5.1 to 1.9.0 is not supported; sequential upgrades through each minor version are required (1.5.1 â†’ 1.6.x â†’ 1.7.x â†’ 1.8.x â†’ 1.9.x).
- Upgrade to Longhorn 1.8.3 failed because the chart version was not available in the repo; upgrade is now proceeding with 1.8.2 (latest available 1.8.x).
- Longhorn 1.8.2 is now deployed, but node k3s1 shows as "down" in UI due to missing engine image readiness and Kubernetes disk pressure (KubeletHasDiskPressure).
- Node disk pressure and "down" state resolved after log cleanup; node is now healthy and ready for next upgrade.
- Longhorn 1.9.0 upgrade is complete, but node is "down" and disks are not available/schedulable; no "Disks" tab in UI; disk cleanup and troubleshooting must continue post-upgrade. (User clarified that disks are intentionally set to unschedulable, which explains lack of available disks.)
- Duplicate sdf1 disks present due to mix of manual and declarative registration; UI is now uneditable for disks. Full cleanup required node removal and disk wipe before re-registration. Node has now been reset, disks wiped, and re-registered declaratively.
- After full node reset and disk wipe, node and Longhorn are back up, but disks are not yet visible in the UIâ€”further investigation required.
- Verified that all disk mount points exist and are empty (only lost+found), Node CR YAML is correct, but disks are still not showing up in Longhorn; root cause under investigation.
- Missing longhorn-disk.cfg files on each disk were identified as the root cause for disks not appearing in Longhorn after disk wipe/reset. Fixing this and restarting Longhorn manager should resolve disk registration.
- All disks now appear in the Longhorn UI after full node reset, disk wipe, disk.cfg recreation, and declarative Node CR re-apply. Full recovery process validated; reconciliation may take 5-10+ minutes.
- HelmRelease values currently do not declare custom disk paths (e.g., /mnt/longhorn/sdf1); this must be set up declaratively for GitOps/idempotency.
- After full recovery, disks are still not visible in the UI and the Longhorn engine image (v1.9.0) is repeatedly failing to deploy; further investigation into engine image readiness and disk registration is required.
- Longhorn UI error: disk is not ready due to 'failed to unmarshal host /mnt/longhorn/sdf1/longhorn-disk.cfg content: : unexpected end of JSON input'â€”disk.cfg must contain valid JSON, not be empty.
- **Critical lesson:** longhorn-disk.cfg must always contain '{}' (valid JSON), never be empty, or disk registration will fail with a JSON parse error.
- After node re-registration, 6 volumes are in "faulted" state with Scheduling Failure / Replica Scheduling Failure (nodes are unavailable). Need to diagnose and recover or clean up these volumes.
- All faulted volumes have been deleted; Longhorn cluster is now clean and stable. Prometheus volume was not in use, so no data loss occurred.
- Prometheus will be used to test new volume provisioning and attachment after cleanup.
- Node labels were missing after node reset, causing replica scheduling failure; labels have now been restored and provisioning should succeed.
- Prometheus volume provisioning and attachment test completed; CSI/Longhorn now functional after full recovery.
- Force cleanup of stuck PVCs/PVs/finalizers was required after CSI/controller disruption; this is a known recovery step after major node/disk/volume resets.
- Persistent error: new Longhorn volumes (e.g. for Prometheus) fail to start replicas with 'missing parameters for replica instance creation'. Root cause is not yet resolved; further investigation required.
- **Note:** Persistent Longhorn replica creation failure error is a critical issue that needs immediate attention. Investigation and root cause analysis are required to resolve this issue.
- **Critical finding:** Longhorn node CRs show missing disk UUIDs for all disks, which blocks replica creation. This must be addressed in the GitOps-managed Node CR or HelmRelease values to ensure proper disk registration.
- **GitOps Remediation Steps:**
  - Review all Longhorn Node CR and HelmRelease YAML in Git for any `diskUUID`, `diskConfig`, or hardcoded disk identifiers. Remove them if present.
  - If Node CR YAML is already correct (no diskUUID fields), re-commit and push to trigger a fresh Flux reconciliation.
  - After reconciliation, verify disk UUIDs are present in the Node CR status. If not, escalate as a Longhorn bug with full YAML and logs.
  - **If disk UUIDs are still missing, perform a full GitOps-compliant node removal, disk wipe, and re-registration:**
    - Remove the Node CR YAML from Git and commit/push to trigger Flux to delete the node from Longhorn.
    - Wait for the node to be removed from the cluster (verify with `kubectl get nodes.longhorn.io -n longhorn-system`).
    - Manually wipe all Longhorn disks on the node (ensure no residual data or config remains except valid longhorn-disk.cfg).
    - Clean up any stuck volumes or PVCs as needed (note: stuck volumes in 'deleting' state will block node removal and must be force-deleted).
    - Re-add the Node CR YAML in Git, commit/push, and verify that disk UUIDs are assigned after reconciliation.
    - Test new volume creation and attachment.
- If node CR is stuck after volume/finalizer cleanup, check for lingering scheduled replicas or CR state drift; force cleanup (e.g., manager restart or repeated finalizer removal) may be required.

## Task List
- [x] Ensure iSCSI services (iscsid/open-iscsi) are enabled and running on k3s1
- [x] Check node labeling for Longhorn (node.longhorn.io/create-default-disk, storage=longhorn)
- [x] Check disk mounts and available space on k3s1
- [x] Investigate and resolve missing large disk mount at /var/lib/longhorn/ on k3s1 (mounted /dev/sdf1 at /mnt/longhorn/sdf1)
- [x] Check permissions and longhorn-disk.cfg presence on /mnt/longhorn/sdf1
- [x] Review Longhorn manager logs for disk errors
- [x] Apply and verify Node CR manifest for disk registration on k3s1
- [x] Troubleshoot and resolve Longhorn disk registration/configuration so disk is schedulable in UI
- [x] Upgrade Longhorn to 1.6.2 and verify stabilization
- [x] Upgrade Longhorn to 1.7.x and verify stabilization
- [x] Upgrade Longhorn to 1.8.x and verify stabilization
- [x] Resolve node "down" state and disk pressure after 1.8.2 upgrade
- [x] Upgrade Longhorn to 1.9.x and verify stabilization
- [x] Resolve duplicate sdf1 disk entries (may require node removal, disk wipe, and re-registration)
- [x] Verify disk registration and cleanup in UI after node reset
- [x] Create longhorn-disk.cfg on each disk and restart Longhorn manager
- [x] Ensure longhorn-disk.cfg contains '{}' (valid JSON) on each disk
- [ ] Review and update HelmRelease or Node CR manifest to declare all three custom disk paths for Longhorn (GitOps/idempotent setup for k3s1 and k3s2)
- [ ] After Longhorn upgrade, re-verify disk cleanup and registration in the improved UI
- [ ] Investigate why Longhorn UI/pods are still version 1.5.1 after HelmRelease upgrade to 1.9.0
- [ ] Troubleshoot node "down" state and disk scheduling in 1.9.0
- [ ] Investigate engine image readiness and disk registration issues post-recovery
- [x] Diagnose and recover/salvage or clean up all faulted volumes (Scheduling Failure / Replica Scheduling Failure)
- [x] Restore Longhorn node labels after node reset to enable scheduling
- [x] Validate Prometheus volume provisioning and attachment after restoring node labels
- [x] Force cleanup of stuck PVCs/PVs/finalizers after CSI/controller disruption
- [ ] Root cause analysis and remediation for Longhorn replica creation failure ('missing parameters for replica instance creation')
- [ ] Investigate and resolve persistent Longhorn replica creation failure error
- [x] Force cleanup of stuck/deleting Longhorn volumes to unblock node removal
- [x] Force cleanup of scheduled replicas/CR state to unblock node removal (e.g., restart Longhorn manager, re-patch finalizer)
- [x] Full GitOps-compliant node removal (CR deleted, confirmed absent)
- [x] Disk wipe on k3s1, validate clean/empty mount points, and re-add Node CR in Git
- [x] Create and apply Flux Kustomization patch for explicit disk annotation on k3s1 (/mnt/longhorn/sdh1)
- [x] Verify node re-registration in Longhorn and disk UUID assignment

## Current Goal
Verify node re-registration and disk UUIDs

---

# GitOps Troubleshooting Plan for Flux CD

## Core GitOps Principles
- **Declarative Configuration**: All configurations are defined in code and stored in Git
- **Versioned and Immutable Storage**: System state is stored in Git with version control
- **Automated Delivery**: Changes in Git trigger automated deployments via Flux
- **Continuous Reconciliation**: Flux maintains cluster state to match Git configuration

## Current Issues
1. **Longhorn UI Access Failure**
   - Cannot access UI via Ingress or NodePort
   - Potential network or service/endpoint synchronization issues

2. **Ingress Conflict for Example App**
   - Host and path already defined in Ingress
   - Causing Kustomization to fail

3. **Monitoring Kustomization Dependency**
   - Stuck due to missing infrastructure Kustomization
   - Previous resolution attempts unsuccessful

## Phase 1: Verify Git Repository and Flux Health
- [x] Verify local Git repository state is clean and up-to-date
  - Repository is up to date with origin/main
  - Untracked files found (docs/implementation-plan.md, prometheus-crd.yaml)
  ```bash
  git status
  git fetch origin
  git pull origin main
  ```
- [x] Check Flux controller pods status
  - All controllers running: source-controller, kustomize-controller, helm-controller, notification-controller
  ```bash
  kubectl get pods -n flux-system
  ```
- [x] Verify Flux system status
  - Flux v2.6.1 (upgrade available to v2.6.2)
  - All controllers and CRDs are healthy
  ```bash
  flux check
  ```
- [x] Inspect core Flux CRDs
  - Git repository: flux-system is ready
  - Kustomization issues found:
    - example-app: Ingress conflict with duplicate host/path
    - monitoring: Missing dependency 'flux-system/infrastructure'
  - Helm releases:
    - nginx-ingress: Ready
    - longhorn: Ready
    - monitoring-kube-prometheus-stack: Failed (chart version issue)
  ```bash
  flux get sources git -A
  flux get kustomizations -A
  flux get helmreleases -A
  ```

## Phase 2: Diagnose Specific Resource Issues
### Longhorn UI Access
- [ ] Review Longhorn HelmRelease in Git
  - Check UI exposure settings and service type
  - Verify Ingress configuration
- [ ] Inspect Flux-managed resources
  ```bash
  kubectl get svc -n longhorn-system longhorn-frontend -o yaml
  kubectl get ingress -A
  kubectl describe helmrelease longhorn -n longhorn-system
  ```

### Example App Ingress Conflict
- [x] Locate conflicting Ingress in Git
  - Found Ingress `example-app` in `default` namespace with host `example-app.local`
  - Source: `apps/example-app/base/ingress.yaml`
  - Kustomization target: `clusters/k3s-flux/example-app-kustomization.yaml`
- [x] Resolve conflict by:
  - [x] Updated the host in the Ingress to `dev.example-app.local`
  - [ ] Removed the existing Ingress (not needed as we updated the host)
- [x] Verify reconciliation
  - Changes committed and pushed to main branch
  - Flux successfully reconciled the Kustomization
  - New Ingress created with host `dev.example-app.local`
  - Old Ingress with `example-app.local` has been removed
  ```bash
  flux reconcile kustomization example-app -n flux-system
  kubectl get ingress -A
  ```

### Monitoring Kustomization
- [x] Check dependsOn configuration in monitoring Kustomization
  - Found monitoring Kustomization at `clusters/k3s-flux/monitoring.yaml`
  - No explicit `dependsOn` field found in the Kustomization
  - The error suggests a dependency on 'flux-system/infrastructure' that's not defined
- [x] Check if infrastructure Kustomization is needed
  - [x] Found infrastructure Kustomization definition at `clusters/k3s-flux/infrastructure.yaml`
  - [x] The infrastructure Kustomization is not currently applied in the cluster
  - [x] Determined the infrastructure Kustomization should be created as it manages core infrastructure components
- [x] Apply the infrastructure Kustomization
  ```bash
  kubectl apply -f clusters/k3s-flux/infrastructure.yaml
  ```
  - [x] Infrastructure Kustomization applied but failing with error about missing file:
    ```
    kustomize build failed: accumulating resources: accumulation err='accumulating resources from 'namespaces/storage-test-namespace.yaml': open /tmp/kustomization-1133777527/infrastructure/namespaces/storage-test-namespace.yaml: no such file or directory'
    ```
  - [x] Reverted the problematic commit that removed the reference to the missing file
  - [x] Discovered failing HelmRelease for monitoring-kube-prometheus-stack:
    ```
    HelmChart 'monitoring/monitoring-monitoring-kube-prometheus-stack' is not ready: 
    invalid chart reference: failed to get chart version for remote reference: 
    no 'kube-prometheus-stack' chart with version matching '55.12.0' found
    ```
  - [x] Updated kube-prometheus-stack chart version from '^55.0.0' to '75.6.0' in `infrastructure/monitoring/prometheus-operator.yaml`
- [x] Next steps:
  - [x] Check if the `storage-test-namespace.yaml` file is actually needed or if it can be safely removed from the Kustomization
    - [x] Found reference to `namespaces/storage-test-namespace.yaml` in `infrastructure/kustomization.yaml`
    - [x] The file doesn't exist in the repository
    - [x] Removed the reference to the non-existent file from the Kustomization
  - [x] Fix the monitoring-kube-prometheus-stack HelmRelease by either:
    - [x] Updated the chart version to an existing version (75.6.0)
    - [ ] Ensuring the Helm repository has the correct chart version available
  - [x] Resolve the missing `storage-test-namespace.yaml` issue in the infrastructure Kustomization by either:
    - [x] Removed the reference from the Kustomization as it appears to be unused
  - [x] Verify the infrastructure Kustomization reconciles successfully before proceeding with monitoring
- [x] Verify reconciliation
  ```bash
  flux reconcile kustomization infrastructure -n flux-system
  flux reconcile kustomization monitoring -n flux-system
  flux get kustomizations -A
  ```
  - [x] Infrastructure Kustomization is in "Reconciliation in progress" state but not completing
  - [ ] Monitoring Kustomization is still waiting for infrastructure dependency
  - [x] Checked kustomize-controller and helm-controller logs for more details
  - [x] Identified that the monitoring Kustomization is waiting for the infrastructure Kustomization to be ready
  - [x] Discovered that the monitoring-kube-prometheus-stack HelmRelease is still failing due to chart version '55.12.0' not being available
  - [x] Found that the HelmRepository is correctly configured and synced
  - [x] Identified multiple HelmRelease configurations causing conflicts:
    - Local configuration: `prometheus-operator` with version '75.6.0'
    - Base configuration: `kube-prometheus-stack` with version '55.12.0'
    - Deployed HelmRelease: `monitoring-kube-prometheus-stack` (using version '55.12.0')
  - [x] Need to update the deployed HelmRelease to use version '75.6.0' to match the available chart version
  - [x] Updated the base HelmRelease configuration in `infrastructure/monitoring/base/helm-release.yaml` to:
    - Use API version `helm.toolkit.fluxcd.io/v2` (from v2beta1)
    - Rename the HelmRelease to `monitoring-kube-prometheus-stack` for consistency
    - Update the chart version to '75.6.0'
  - [x] Committed and pushed the changes to the Git repository
  - [x] Verified the HelmRelease is being installed with the updated chart version (75.6.0)
  - [x] Monitor the HelmRelease installation progress (currently running with 10m timeout)
  - [x] Verify the HelmRelease installation is in progress but timing out
  - [x] Checked helm-controller logs for installation progress
  - [x] Identified that the HelmRelease installation is failing with timeout errors
  - [x] Observed that the HelmRelease is stuck in a loop of installation attempts
  - [x] Found that the infrastructure Kustomization is failing due to the HelmRelease timeout
  - [x] Confirmed that the HelmRelease should complete much faster than the timeout suggests
  - [x] Checked HelmChart resources and found none in the monitoring namespace
  - [x] Identified from events and logs that the monitoring HelmRelease is stuck due to PersistentVolumeClaims (PVCs) not being bound or volumes not being ready (storage/Longhorn issue)
  - [x] User reported Longhorn nodes are "unschedulable" and disks are not visible in the UI
  - [ ] Next steps to resolve the timeout issue:
    - [ ] Check for any stuck or pending resources in the monitoring namespace
    - [ ] Verify the Helm repository configuration and connectivity
    - [ ] Consider deleting and recreating the HelmRelease to reset its state
    - [ ] Check for any admission webhooks or policies that might be interfering
    - [ ] Diagnose and fix the underlying storage/PVC/volume provisioning issue (Longhorn or CSI)
    - [ ] Consult and follow steps in docs/longhorn-setup.md and related docs to resolve Longhorn node/disk configuration (unschedulable nodes, missing disks)

## Current Goal
Fix the underlying storage/PVC/volume issue (Longhorn/CSI) preventing monitoring HelmRelease from succeeding.
- [ ] Consult docs/longhorn-setup.md and related docs to resolve Longhorn node/disk configuration (unschedulable nodes, missing disks)

## Phase 3: Deep Dive & Remediation
- [x] Review controller logs for errors
  - [x] Checked kustomize-controller logs
  - [x] Found deprecation warnings for Helm API versions
  - [x] Identified that infrastructure Kustomization is making progress but not completing
  - [ ] Check for any resource conflicts or missing dependencies
  ```bash
  kubectl logs -n flux-system -l app.kubernetes.io/name=flux -c manager
  kubectl logs -n flux-system -l app=helm-controller
  kubectl logs -n flux-system -l app=source-controller
  kubectl logs -n kube-system -l component=kube-proxy
  ```

## Phase 4: Verification & Preventative Measures
- [ ] Verify full reconciliation
  ```bash
  flux check
  flux get all -A
  kubectl get pods -n flux-system
  kubectl get pods -n longhorn-system
  kubectl get svc -n longhorn-system longhorn-frontend
  kubectl get ingress -A
  kubectl get pods -l app=example-app
  ```
- [ ] Test Longhorn UI access
  ```bash
  kubectl port-forward svc/longhorn-frontend 8080:80 -n longhorn-system
  ```
  - Access via: http://192.168.86.71:30080/longhorn

---

# K3s Homelab GitOps Strategy with Flux

## Notes
- Using Windsurf for project management
- Implementing GitOps with Flux CD for declarative cluster management
- K3s as the lightweight Kubernetes distribution
- Monorepo structure for all configurations
- Security-first approach with SOPS for secrets management
- Automated image updates with Flux image automation
- Cluster nodes:
  - k3s1: 192.168.86.71 (primary, live)
  - k3s2: 192.168.86.72 (future node)

## Phase 1: Core Infrastructure Setup
- [x] Initialize Git repository and connect to GitHub
- [x] Set up k3s cluster
  - [x] Install k3s on k3s1 (192.168.86.71)
  - [x] Configure kubeconfig for remote access
  - [x] Verify cluster access
  - [ ] Prepare k3s2 (192.168.86.72) for future expansion
- [x] Bootstrap Flux CD
  - [x] Install Flux CLI
  - [x] Run pre-flight checks
  - [x] Execute bootstrap command
  - [x] Verify Flux installation
  - [x] Confirm flux-system Kustomization is healthy

## Phase 2: Repository Structure
- [x] Create base directory structure
  - [x] clusters/k3s-flux/flux-system/
  - [x] infrastructure/base/
  - [x] apps/_templates/base/
- [x] Set up initial Kustomize configurations
- [x] Create initial Flux Kustomization for infrastructure
- [x] Commit and push repository structure

## Phase 3: Infrastructure Components
- [x] Set up Ingress Controller (Nginx)
  - [x] Create Nginx Ingress Kustomization
  - [x] Configure HelmRelease with ServiceMonitor disabled
  - [x] Update HelmRelease to use NodePort (30080/30443)
  - [x] Verify Nginx Ingress installation
    - [x] Check Helm release status
    - [x] Verify pod deployment
    - [x] Check service creation (NodePort 30080/30443)
    - [x] Test basic HTTP access (404 response confirms Nginx is running)
  - [x] Deploy default backend for Nginx Ingress
  - [x] Configure default backend health checks
    - [x] Update deployment to use TCP probes
    - [x] Test health check functionality
    - [x] Document the configuration
  - [x] Set up basic routing rules
    - [x] Create example application deployment
    - [x] Create Ingress resource for the example application
    - [x] Test routing to the example application
  - [ ] (Future) Enable ServiceMonitor in Phase 6 with Prometheus

## Phase 4: Application Management (Detailed Implementation)

### 4.1. Define Application Base Configuration with Kustomize
- [x] Create base directory structure for example-app
  - [x] `apps/example-app/base/`
  - [x] Core Kubernetes manifests (deployment.yaml, service.yaml)
  - [x] Base kustomization.yaml
- [x] Configure common labels and annotations
- [x] Set up health checks and resource limits

### 4.2. Implement Environment-Specific Overlays
- [x] Create overlay directories:
  - [x] `apps/example-app/overlays/dev/`
  - [x] `apps/example-app/overlays/staging/`
  - [x] `apps/example-app/overlays/prod/`
- [x] Configure environment-specific settings:
  - [x] Replica counts
  - [x] Resource limits
  - [x] Environment variables
  - [x] Image tags

### 4.3. Configure Flux Kustomization CRDs
- [x] Create Flux Kustomization for dev environment
  - [x] `clusters/k3s-flux/apps-example-app-dev.yaml`
  - [x] Set interval: 1m
  - [x] Enable pruning
- [x] Create Flux Kustomization for staging environment
  - [x] `clusters/k3s-flux/apps-example-app-staging.yaml`
  - [x] Set interval: 5m
  - [x] Enable pruning
- [x] Create Flux Kustomization for production environment
  - [x] `clusters/k3s-flux/apps-example-app-prod.yaml`
  - [x] Set interval: 10m
  - [x] Enable pruning

### 4.4. Implement "Offline until Needed" Functionality
- [x] Configure scaling to zero for non-production environments
- [x] Document process for suspending/resuming reconciliation
- [x] Test environment suspension and resumption

### 4.5. Set Up Dependency Management with dependsOn
- [x] Identify and document application dependencies
- [x] Configure dependsOn in Kustomization CRDs
- [x] Test deployment order and dependencies

### 4.6. Enable Pruning for Clean Resource Cleanup
- [x] Enable pruning in all Kustomization CRDs
- [x] Test resource cleanup
- [x] Document pruning behavior

### 4.7. Namespace Management
- [x] Create dedicated namespaces for each environment
  - [x] example-app-dev
  - [x] example-app-staging
  - [x] example-app-prod
- [x] Configure namespace-specific resource quotas and limits

### 4.8. Documentation and Verification
- [x] Document the application deployment workflow
- [x] Create runbooks for common operations
- [x] Verify all environments are functioning correctly

### 4.9. Security Considerations
- [x] Implement network policies
- [x] Configure RBAC for each environment
- [x] Set up resource quotas and limits

### 4.10. Monitoring and Observability
- [x] Configure logging for application components
- [x] Set up basic metrics collection
- [x] Document monitoring approach

### 4.11. Testing Strategy
- [x] Unit testing for Kustomize overlays
- [x] Integration testing for environment-specific configurations
- [x] End-to-end testing of deployment workflow

### 4.12. Rollback Procedures
- [x] Document rollback procedures for each environment
- [x] Test rollback scenarios
- [x] Implement automated health checks

### 4.13. Performance Optimization
- [x] Optimize container resource requests/limits
- [x] Configure horizontal pod autoscaling for production
- [x] Implement pod disruption budgets

### 4.14. Disaster Recovery
- [x] Document recovery procedures
- [x] Test backup and restore processes
- [x] Implement automated backup solutions

### 4.15. Documentation
- [x] Update README with application deployment instructions
- [x] Document environment-specific configurations
- [x] Create troubleshooting guide

## Phase 5: Storage with Longhorn

### 5.1. k3s1 Node Disk Preparation (Completed)
- [x] SSH into k3s1: `ssh user@192.168.86.71`
- [x] Identify available disks: `lsblk -o NAME,SIZE,TYPE,MOUNTPOINT,FSTYPE,MODEL`
  - Identified disks: sdf, sdg, sdh (465.8GB each)
- [x] Document current disk configuration
- [x] Prepare disks for Longhorn (destructive operation):
  - [x] Remove existing partitions: `sudo sgdisk --zap-all /dev/sdX`
  - [x] Wipe filesystem signatures: `sudo wipefs -a /dev/sdX`
  - [x] Create GPT partition table: `sudo parted /dev/sdX mklabel gpt --script`
  - [x] Create primary partition: `sudo parted -a opt /dev/sdX mkpart primary 0% 100%`
  - [x] Format partition: `sudo mkfs.ext4 /dev/sdX1`
- [x] Create mount points: `/mnt/longhorn/disk{f,g,h}`
- [x] Configure `/etc/fstab` for persistent mounts
- [x] Label node for Longhorn auto-discovery:
  ```bash
  kubectl label node k3s1 node.longhorn.io/create-default-disk=config
  kubectl label node k3s1 storage=longhorn
  ```

### 5.2. Enhanced k3s2 Node Provisioning
- [x] Cloud-init configuration for automated provisioning
  - [x] Raw disk preparation for Longhorn (no filesystem)
  - [x] DNS-based K3s master discovery (k3s1.local)
  - [x] Secure token management with SOPS
  - [x] Automated Longhorn disk configuration via CRD
  - [x] Idempotent operations for reliability

### 5.3. Cloud-init Server (Deployed)
- [x] Nginx server for serving cloud-init configuration
  - [x] ConfigMap for cloud-init data
  - [x] NodePort service (30090)
- [x] CoreDNS for local DNS resolution
  - [x] k3s1.local â†’ 192.168.86.71
- [x] Secure token management
  - [x] SOPS-encrypted K3s token
  - [x] Secure token retrieval in cloud-init

### 5.4. Verification & Testing

---

### 5.5. Longhorn Upgrade, Disk Cleanup, and Recovery (2025-06)

- **Sequential Upgrade Path:**
  - Upgraded Longhorn via HelmRelease sequentially: 1.5.1 â†’ 1.6.2 â†’ 1.7.2 â†’ 1.8.2 â†’ 1.9.0.
  - Skipped unavailable chart versions (e.g., 1.8.3) when necessary.
  - Verified pod image versions and UI after each upgrade step.
- **Disk Management Lessons:**

---

### 5.6. GitOps-First Longhorn Troubleshooting & Remediation (2025-06)

#### Node Removal and Disk Wipe: What Worked and What Didn't (2025-06)

**What Didn't Work:**
- Removing the finalizer from the Node CR (`kubectl patch ...`) did not remove the node.
- Restarting the Longhorn manager pod did not remove the node.
- Disk wipe commands failed while disks were still mounted or in use.
- Attempting to format a partition before creating it resulted in errors.

**What Worked:**
- Deleting all stuck Longhorn volumes and removing their finalizers.
- Forcibly deleting the Node CR (`kubectl delete --force --grace-period=0 ...`) finally removed the node from Longhorn.
- Unmounting disks and running `partprobe` before wiping allowed for successful disk zapping and re-partitioning.
- Creating a new GPT partition and primary partition with `parted` before formatting.
- Formatting partitions as ext4 and mounting them to the correct mount points.
- Creating a valid `longhorn-disk.cfg` containing `{}` on each disk.

**Lessons Learned:**
- Node CR removal may require force deletion even after all volumes and finalizers are gone.
- Always unmount disks and run `partprobe` before attempting to wipe or re-partition.
- Partition creation must precede formatting.
- GitOps-driven re-addition is only reliable after a truly clean node and disk state.

- **Strict GitOps Principles:**
  - All persistent changes must be made in Git and reconciled by FluxCD. Avoid direct kubectl edits except for diagnostics or emergency stuck resource cleanup, and always update Git to reflect the desired state.
- **Troubleshooting Steps:**
  1. **Verify Flux and Cluster Health:**
     - Use `flux check`, `kubectl get pods -n flux-system`, and `flux get kustomizations` to confirm Flux and Git reconciliation health.
     - Ensure all nodes and system pods are Ready.
  2. **Network and CNI:**
     - Confirm CNI pods are healthy and pod-to-pod networking works.
  3. **Longhorn HelmRelease and Values:**
     - Review and update `infrastructure/longhorn/base/helm-release.yaml` for correct disk selectors, data paths, and node labels.
     - All custom disk config, node labels, and storage class settings must be GitOps-managed.
  4. **Longhorn Control Plane and CSI Health:**
     - Ensure all Longhorn pods (manager, csi-plugin, etc.) are Running.
     - Confirm Longhorn CRDs and StorageClass are present and correct.
  5. **Longhorn Node and Disk State:**
     - Use `kubectl get nodes.longhorn.io -n longhorn-system` and `kubectl describe nodes.longhorn.io <node>` to verify disks are registered, healthy, and schedulable.
     - Disks must show as Ready and Schedulable in both Git and the cluster.
  6. **Volume and Replica Diagnostics:**
     - Use `kubectl describe volume <vol>` and review Longhorn manager/CSI logs for errors.
     - If volumes are stuck (e.g., 'attaching'), check for disk registration, node health, and RBAC.
  7. **RBAC Verification:**
     - Ensure all RBAC for Longhorn and CSI is managed in Git and grants necessary permissions.
  8. **Remediation:**
     - If stuck resources require manual cleanup (kubectl delete volumeattachment, PV, etc.), immediately update Git to reflect the intended state.
     - For configuration issues, always fix the HelmRelease or Kustomization YAML in Git and reconcile via Flux.
- **Persistent Error Note:**
  - If you encounter 'missing parameters for replica instance creation' and all disk configs, labels, and node health are correct, this indicates a deeper Longhorn bug or CRD state drift. Document the error, escalate for further root cause analysis, and avoid manual fixes that would break GitOps drift detection.

> **Warning:** Never bypass GitOps for persistent configuration. Use kubectl only for diagnostics or emergency stuck resource cleanup, and always update Git to match the intended state.

---
  - Always use declarative Node CR YAML for disk registration (GitOps/idempotency).
  - Manual disk registration in UI causes duplicates and UI lockout.
  - If duplicate disks appear, full cleanup requires:
    1. Disabling scheduling and evicting all replicas from the disk.
    2. Removing the disk from the Node CR and applying.
    3. If stuck, delete the Node CR (node removal), wipe/reformat disks, remove longhorn-disk.cfg, and re-register declaratively.
    4. Ensure longhorn-disk.cfg exists on each disk mount after wipe/reformat.
    5. Restart longhorn-manager pods to force reconciliation.
  - Disk registration can take 5-10+ minutes after full reset.
- **Recovery Process (Validated):**
  - Node removal, disk wipe, and declarative re-registration restores a clean state.
  - All disks reappear as healthy and schedulable with no duplicates.
- **Permanent Documentation:**
  - See `docs/k3s-flux-longhorn-guide.md` and `docs/longhorn-setup.md` for full, step-by-step procedures and troubleshooting.

---
- [ ] Test k3s2 provisioning
  - [ ] Boot with cloud-init URL: `http://192.168.86.71:30090/k3s2`
  - [ ] Verify node joins cluster
  - [ ] Check Longhorn disk detection
  - [ ] Test volume scheduling
- [ ] Validate cross-node functionality
  - [ ] Create pod on k3s2 with Longhorn volume
  - [ ] Verify data persistence
  - [ ] Test pod migration between nodes

### 5.5. Documentation & Recovery
- [x] Document disk preparation process
- [x] Create node provisioning guide
- [x] Document recovery procedures
  - [x] Master node recovery
  - [x] Worker node reprovisioning
  - [x] Full cluster rebuild
- [x] Update infrastructure diagrams
- [x] Document token rotation process

### 5.3. Longhorn Configuration
- [x] Create Longhorn namespace and RBAC
- [x] Configure Longhorn Helm repository with version pinning (1.5.x)
- [x] Deploy Longhorn using HelmRelease with production-ready values:
  ```yaml
  defaultSettings:
    createDefaultDiskLabeledNodes: true
    defaultDataPath: /var/lib/longhorn/
    defaultDiskSelector: ["storage=longhorn"]
    replicaSoftAntiAffinity: true
    replicaZoneSoftAntiAffinity: true
  ```
  - [x] Configure default storage settings
  - [x] Set resource requests/limits
  - [x] Enable auto-upgrades for patch versions
  - [x] Configure pruning and dependency management

### 5.4. Verification and Testing (Completed)
- [x] Verify Longhorn installation:
  - [x] Check all Longhorn pods are running: `kubectl get pods -n longhorn-system`
  - [x] Verify CRDs are installed: `kubectl get crd | grep longhorn`
  - [x] Access Longhorn UI via port-forward: `kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80`
- [x] Test storage functionality:
  - [x] Create test PVC and verify binding
  - [x] Deploy test pod to write/read data
  - [x] Verify data persistence across pod restarts
  - [x] Test volume expansion
  - [x] Verify replica scheduling across disks
- [x] Document the setup in `docs/k3s-flux-longhorn-guide.md`
  - [ ] Verify CRDs are installed: `kubectl get crd | grep longhorn`
  - [ ] Check Longhorn UI accessibility
- [ ] Test storage functionality:
  - [ ] Create test PVC and verify binding
  - [ ] Verify data persistence across pod restarts
  - [ ] Test volume expansion
  - [ ] Verify replica scheduling across nodes
  - [ ] Test node failure scenarios

### 5.5. Multi-Node Configuration
- [ ] Configure node affinity and anti-affinity
- [ ] Set up storage classes for different disk types
- [ ] Configure volume attachment recovery policy
- [ ] Test cross-node volume migration

### 5.6. Backup Configuration
- [ ] Set up backup target (NFS/S3):
  - [ ] Configure backup credentials using SOPS
  - [ ] Test backup/restore procedures
  - [ ] Schedule regular backups
  - [ ] Document recovery process
  - [ ] Set up backup retention policies

### 5.7. Monitoring and Alerting
- [ ] Configure Prometheus ServiceMonitor for Longhorn
- [ ] Import Longhorn Grafana dashboards
- [ ] Set up critical alerts for:
  - [ ] Volume health and status
  - [ ] Storage capacity and usage
  - [ ] Backup failures and consistency
  - [ ] Node disk pressure and I/O performance
  - [ ] Replication status and consistency

### 5.8. Documentation and Runbooks
- [ ] Document disk preparation process
- [ ] Create node provisioning guide
- [ ] Document recovery procedures
- [ ] Create troubleshooting guide
- [ ] Document performance tuning recommendations

### 5.9. Future Enhancements
- [ ] Implement backup target rotation
- [ ] Set up disaster recovery procedures
- [ ] Configure storage network isolation
- [ ] Implement storage policies for different workloads
- [ ] Create troubleshooting guide
- [ ] Document backup/restore procedures
- [ ] Document upgrade procedures

### 5.7. Integration with Other Components
- [ ] Configure StorageClass as default
- [ ] Update application manifests to use Longhorn storage
- [ ] Test application failover scenarios
- [ ] Document storage requirements for applications

## Phase 6: Security & Secrets
- [ ] Implement SOPS for secrets management
  - [ ] Set up encryption keys
  - [ ] Configure Flux for decryption
  - [ ] Encrypt existing secrets
- [ ] Configure RBAC and network policies
- [ ] Set up workload identity for cloud services

## Phase 6: Automation & Monitoring
- [ ] Configure image automation
  - [ ] Set up image repositories
  - [ ] Define update policies
  - [ ] Enable automated PRs for updates
- [ ] Implement monitoring and alerting
  - [ ] Deploy Prometheus stack
  - [ ] Set up Grafana dashboards
  - [ ] Configure alerts

## Current Goal
Complete Longhorn Distributed Storage Implementation

## Implementation Strategy

### 1. Repository Structure
```
infrastructure/
  longhorn/
    base/
      kustomization.yaml
      namespace.yaml
      helm-repository.yaml
      helm-release.yaml
      backup-secret.sops.yaml  # Encrypted credentials
    overlays/
      production/
        kustomization.yaml
        values-override.yaml
```

### 2. Key Configuration Files

#### 2.1. HelmRepository
```yaml
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: longhorn
  namespace: infrastructure
spec:
  interval: 1h
  url: https://charts.longhorn.io
  type: oci
  interval: 1h
```

#### 2.2. HelmRelease (Key Components)
```yaml
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: longhorn
  namespace: longhorn-system
spec:
  interval: 1h
  chart:
    spec:
      chart: longhorn
      version: 1.5.x  # Auto-update patch versions
      sourceRef:
        kind: HelmRepository
        name: longhorn
        namespace: infrastructure
      interval: 1h
  values:
    defaultSettings:
      defaultDataPath: /var/lib/longhorn/
      defaultReplicaCount: 2
      defaultDataLocality: best-effort
      replicaSoftAntiAffinity: true
      replicaZoneSoftAntiAffinity: true
      createDefaultDiskLabeledNodes: true
      defaultDiskSelector: ["storage=longhorn"]
      backupTarget: "s3://backup-bucket@us-east-1/"
      backupTargetCredentialSecret: "longhorn-backup-credentials"
    persistence:
      defaultClass: true
      defaultFsType: ext4
```

### 3. Verification Steps

#### 3.1. Check Installation
```bash
# Verify Helm release
flux get helmreleases -n longhorn-system

# Check pods
kubectl get pods -n longhorn-system

# Verify storage classes
kubectl get storageclass
```

#### 3.2. Test Storage
```yaml
# test-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 1Gi
```

### 4. Troubleshooting Guide

#### 4.1. Common Issues
- **Pods not starting**: Check node requirements and resource constraints
- **Volume attachment failures**: Verify network connectivity between nodes
- **Backup failures**: Check credentials and network access to backup target

#### 4.2. Diagnostic Commands
```bash
# Check Longhorn manager logs
kubectl logs -n longhorn-system -l app=longhorn-manager

# Describe failing pods
kubectl describe pod -n longhorn-system <pod-name>

# Check volume status
kubectl get volumes -n longhorn-system
```

## Next Steps

### 1. Deploy Cloud-init Server
```bash
# Deploy the cloud-init server
kubectl apply -k infrastructure/cloud-init

# Verify deployment
kubectl get pods,svc -n cloud-init
```

### 2. Provision k3s2 Node
1. Boot k3s2 with cloud-init URL: `http://192.168.86.71:30090/k3s2`
2. Monitor k3s2 joining the cluster:
   ```bash
   watch kubectl get nodes
   ```
3. Verify Longhorn disk detection:
   ```bash
   kubectl get nodes.longhorn.io -n longhorn-system
   ```

### 3. Test Cross-Node Functionality
- [ ] Schedule test pod on k3s2 with Longhorn volume
- [ ] Verify data persistence across pod rescheduling
- [ ] Test volume migration between nodes

### 4. Monitoring & Observability
- [ ] Deploy Prometheus stack with Flux
- [ ] Configure Longhorn metrics collection
- [ ] Set up Grafana dashboards for storage monitoring
- [ ] Configure alerts for storage capacity and health

### 5. Network Configuration & Access

#### Current Network Setup
- **k3s1 (192.168.86.71)**
  - Control Plane + Worker Node
  - NGINX Ingress Controller (NodePort 30080/30443)
  - Cloud-init server (NodePort 30090)
  - Longhorn UI (via NGINX Ingress at `/longhorn`)

#### Working Services
1. **NGINX Ingress Controller**
   - HTTP: `http://192.168.86.71:30080`
   - HTTPS: `https://192.168.86.71:30443`

2. **Longhorn UI**
   - Access: `http://192.168.86.71:30080/longhorn`
   - Service: `longhorn-frontend` in `longhorn-system` namespace

3. **Cloud-init Server**
   - Access: `http://192.168.86.71:30090`

#### Lessons Learned
1. **What Worked**
   - Using NGINX Ingress for all HTTP/HTTPS traffic
   - Simple path-based routing (`/longhorn`)
   - Minimal configuration changes

2. **What Didn't Work**
   - Direct NodePort configuration was problematic
   - Modifying k3s server config didn't resolve NodePort issues
   - Complex Ingress configurations caused more problems

#### Next Steps
1. **Security**
   - [ ] Add authentication to Longhorn UI
   - [ ] Set up TLS certificates for HTTPS
   - [ ] Implement network policies

2. **Documentation**
   - [ ] Document service endpoints
   - [ ] Add access instructions
   - [ ] Document troubleshooting steps

3. **Backup & Recovery**
   - [ ] Configure Longhorn backup target (S3/NFS)
   - [ ] Set up scheduled backups for critical volumes
   - [ ] Document restore procedures
- [ ] Test backup and restore process